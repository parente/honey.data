{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query prototyping\n",
    "\n",
    "This notebook computes interesting metrics from the raw wheel rotation data stored in S3. It's meant as a prototype for queries that'll run to produce summary data for use by a web frontend later on.\n",
    "\n",
    "An earlier version of the notebook worked with the raw rotation CSV files locally using pandas. Ultimately, I want to keep the compute close to the data in S3 and avoid paying for extra compute elsewhere (e.g., Lambda, ECS). Therefore, this notebook uses Athena, under the assumption that the Raspberry Pi doing the data collection can also trigger Athena queries to produce the summary data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "import math\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import pytz\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants to convert [wheel rotations](https://www.amazon.com/gp/product/B019RH7PPE/ref=ppx_yo_dt_b_asin_title_o04_s00?ie=UTF8&psc=1) into distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheel_diameter = 8.5 # inches, not quite the 9\" advertised, I measured\n",
    "wheel_circumference = math.pi * wheel_diameter / 12 / 5280 # miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client('athena')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll execute Athena queries using boto3. I'm not using `pyathena` to keep demands on the Raspberry Pi light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(query, max_checks=30):\n",
    "    \"\"\"Executes an Athena query, waits for success or failure, and returns the first page\n",
    "    of the query results.\n",
    "    \n",
    "    Waits up to max_checks * 10 seconds for the query to complete before raising.\n",
    "    \"\"\"\n",
    "    resp = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': 'honey_data'\n",
    "        },\n",
    "        WorkGroup='honey-data'\n",
    "    )\n",
    "    qid = resp['QueryExecutionId']\n",
    "    for i in range(max_checks):\n",
    "        resp = athena.get_query_execution(\n",
    "            QueryExecutionId=qid\n",
    "        )\n",
    "        state = resp['QueryExecution']['Status']['State']\n",
    "        if state == 'SUCCEEDED':\n",
    "            return qid, athena.get_query_results(QueryExecutionId=qid)\n",
    "        elif state == 'FAILED':\n",
    "            return qid, resp\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        raise RuntimeError('Reached max_checks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish(qid, s3_key):\n",
    "    \"\"\"Copies Athena results to the public bucket for client use.\"\"\"\n",
    "    return s3.copy_object(\n",
    "        CopySource=f'honey-data/athena-results/{qid}.csv',\n",
    "        Bucket='honey-data-public',\n",
    "        Key=s3_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_partitions():\n",
    "    \"\"\"Update daily partitions.\"\"\"\n",
    "    return q('msck repair table incoming_rotations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_partitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How far has Honey run since we started tracking?\n",
    "\n",
    "This can serve as input to the geopoint API naming a city she could have reached if she traveled this far in straight line distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid, resp = q('''\n",
    "select sum(rotations) total\n",
    "from incoming_rotations\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web client will end up using published CSVs instead of results fetched using the API. Therefore, I'm not investing in data type parsing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_miles = int(resp['ResultSet']['Rows'][1]['Data'][0]['VarCharValue']) * wheel_circumference\n",
    "total_miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the most recent activity available?\n",
    "\n",
    "Not sure this is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q(f'''\n",
    "select max(from_iso8601_timestamp(datetime)) as most_recent_activity\n",
    "from incoming_rotations \n",
    "where year = year(current_date) and month >= month(current_date)-1\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How has she progressed over the last 7 days? (cumulative plot, origin at sum before period)\n",
    "\n",
    "This should be the total sum prior to the window of interest. The query scans everything instead of trying to skip scanning 7 day partitions--a drop in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid, resp = q('''\n",
    "select sum(rotations) as prior_rotations\n",
    "from incoming_rotations\n",
    "where from_iso8601_timestamp(datetime) < (current_date - interval '7' day)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish(qid, 'prior-7-day-window.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this should be the sum of rotations by hour and the cumulative sum by hour within the window of interest. I'm trying to constrain the search space for the necessary data using partitions. I need a bit more data to make sure this is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid, resp = q(f'''\n",
    "select\n",
    "    sum(rotations) as sum_rotations,\n",
    "    to_iso8601(date_trunc('hour', from_iso8601_timestamp(datetime))) as datetime_hour,\n",
    "    sum(sum(rotations)) over (\n",
    "        order by date_trunc('hour', from_iso8601_timestamp(datetime)) asc \n",
    "        rows between unbounded preceding and current row\n",
    "    ) as cumsum_rotations\n",
    "from incoming_rotations\n",
    "where \n",
    "    year >= year(current_date)-1 and\n",
    "    from_iso8601_timestamp(datetime) >= (current_date - interval '7' day)\n",
    "group by date_trunc('hour', from_iso8601_timestamp(datetime))\n",
    "order by datetime_hour\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish(qid, '7-day-window.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the CSV forms of these metrics to create a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = pytz.timezone('America/New_York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.get_object(Bucket='honey-data-public', Key='prior-7-day-window.csv')\n",
    "prior_df = pd.read_csv(resp['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    offset = prior_df.iloc[0].iloc[0]\n",
    "except:\n",
    "    offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = s3.get_object(Bucket='honey-data-public', Key='7-day-window.csv')\n",
    "week_df = pd.read_csv(resp['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df['datetime_utc'] = pd.to_datetime(week_df.datetime_hour)\n",
    "week_df['datetime'] = week_df.datetime_utc.dt.tz_convert(tz)\n",
    "week_df.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values is something I might want to do in Athena instead of relying on the frontend web client doing it if plot interpolation doesn't look pretty. Some techniques here: https://www.reddit.com/r/SQL/comments/80t1db/inserting_dates_between_a_start_date_and_enddate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_df = week_df[['cumsum_rotations']] + offset\n",
    "#cumsum_df = cumsum_df.reindex(pd.date_range(week_df.index.min(), week_df.index.max(), freq='1h'), method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_df.index.max() - cumsum_df.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 5))\n",
    "(cumsum_df * wheel_circumference).rename(columns={'cumsum_rotations': 'miles'}).plot(ax=ax)\n",
    "ax.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df.loc['2020-09-05 12:00:00':'2020-09-06 12:00:00'].sum_rotations.sum() * wheel_circumference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How far has she run each night for the past year?\n",
    "\n",
    "We should subtract 12 hours to sum rotations for nocturnal sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid, resp = q(f'''\n",
    "select\n",
    "    sum(rotations) as sum_rotations,\n",
    "    date(date_trunc('day', from_iso8601_timestamp(datetime) - interval '12' hour)) as date\n",
    "from incoming_rotations\n",
    "where \n",
    "    year >= year(current_date)-1 and\n",
    "    date(date_trunc('day', from_iso8601_timestamp(datetime) - interval '12' hour)) >= current_date - interval '1' year\n",
    "group by date_trunc('day', from_iso8601_timestamp(datetime) - interval '12' hour)\n",
    "order by date\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish(qid, '1-year-window.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What city might she have reached by traveling this distance?\n",
    "\n",
    "https://rapidapi.com/wirefreethought/api/geodb-cities?endpoint=5aadab87e4b00687d35767b4 allows 1000 request per day. If the data upload / aggregation job runs every 10 minutes, I only need about a tenth of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapid_key = getpass.getpass('Rapid API key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durham_lat = '35.994034'\n",
    "durham_lon = '-78.898621'\n",
    "rapid_url = \"https://wft-geo-db.p.rapidapi.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def furthest_poi(lat, lon, radius, api_key, base_url=rapid_url):\n",
    "    path = f'/v1/geo/locations/{durham_lat_lon}/nearbyCities'\n",
    "\n",
    "    # Results sort nearest to farthest\n",
    "    resp = requests.get(\n",
    "        f'{base_url}{path}',\n",
    "        headers = {\n",
    "            'x-rapidapi-host': \"wft-geo-db.p.rapidapi.com\",\n",
    "            'x-rapidapi-key': api_key\n",
    "        }, \n",
    "        params={\"radius\": radius}\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Navigate to the last page\n",
    "    for link in resp.json()['links']:\n",
    "        if link['rel'] == 'last':\n",
    "            path = link['href']\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError('Link to last result page not found')\n",
    "        \n",
    "    resp = requests.get(\n",
    "        f'{base_url}{path}',\n",
    "        headers = {\n",
    "            'x-rapidapi-host': \"wft-geo-db.p.rapidapi.com\",\n",
    "            'x-rapidapi-key': api_key\n",
    "        }, \n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    # Furthest point of interest within the given radius\n",
    "    return resp.json()['data'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "furthest_poi(durham_lat, durham_lon, total_miles, rapid_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\n",
    "    f'{rapid_url}{path}',\n",
    "    headers = {\n",
    "        'x-rapidapi-host': \"wft-geo-db.p.rapidapi.com\",\n",
    "        'x-rapidapi-key': rapid_key\n",
    "    }, \n",
    "    params={\"radius\": total_miles}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in resp.json()['links']:\n",
    "    if link['rel'] == 'last':\n",
    "        path = link['href']\n",
    "        break\n",
    "else:\n",
    "    raise ValueError('Link to last result page not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\n",
    "    f'{rapid_url}{path}',\n",
    "    headers = {\n",
    "        'x-rapidapi-host': \"wft-geo-db.p.rapidapi.com\",\n",
    "        'x-rapidapi-key': rapid_key\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.json()['data'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can turn the `wikiDataId` into a link to a Wikipedia page by querying the Wikidata API followed by the Wikipedia API.\n",
    "\n",
    "* https://www.wikidata.org/w/pi.php?action=wbgetentities&format=json&props=sitelinks&ids=Q1373463&sitefilter=enwiki\n",
    "* https://en.wikipedia.org/w/api.php?action=query&titles=Fries,%20Virginia&format=json\n",
    "* https://en.wikipedia.org/w/api.php?action=query&prop=info&pageids=137620&inprop=url&format=json\n",
    "* https://en.wikipedia.org/wiki/Fries,_Virginia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
